{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/olumide/Documents/Self Improvement - ML/MLapps/credit_worthiness_checker\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from src.model_dev.eval import eval_with_auc_and_pr_curve\n",
    "from src.model_dev.eval import eval_with_average_precision_score\n",
    "from src.model_dev.eval import eval_with_f_beta_score\n",
    "import mlflow\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, input_path:str) -> None:\n",
    "        self.input_path = input_path\n",
    "        self.train_data = None\n",
    "        self.dev_data = None\n",
    "        self.model = None\n",
    "        self.metrics = None\n",
    "        \n",
    "    def _test_manual_downsampling(self, data:pd.DataFrame) -> pd.DataFrame:\n",
    "        data_majority = data[data[\"target\"] == 0]\n",
    "        data_minortiy = data[data[\"target\"] == 1]\n",
    "        df_majority_downsampled = resample(data_majority,\n",
    "                                           replace=False,\n",
    "                                           n_samples = 3*len(data_minortiy),\n",
    "                                           random_state=42)\n",
    "        df = pd.concat([df_majority_downsampled, data_minortiy])\n",
    "        return df\n",
    "    \n",
    "    def _test_manual_upsampling(self, data:pd.DataFrame) -> pd.DataFrame:\n",
    "        data_majority = data[data[\"target\"] == 0]\n",
    "        data_minortiy = data[data[\"target\"] == 1]\n",
    "        df_minortiy_upsampled = resample(data_minortiy,\n",
    "                                         replace=True,\n",
    "                                         n_samples = len(data_majority),\n",
    "                                         random_state=42)\n",
    "        df = pd.concat([data_majority, df_minortiy_upsampled])\n",
    "        return df\n",
    "\n",
    "    def _split_data_into_features_target(self, data:pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        features = data.drop(columns=[\"target\"])\n",
    "        target = data[\"target\"]\n",
    "        return features, target\n",
    "\n",
    "    def load_data(self, train_name:str, dev_name:str) -> pd.DataFrame:\n",
    "        try:\n",
    "            self.train_data = pd.read_parquet(self.input_path + '/' + train_name)\n",
    "            self.dev_data = pd.read_parquet(self.input_path + '/' + dev_name)\n",
    "            #self.train_data = self._test_manual_downsampling(self.train_data)\n",
    "            self.train_data = self._test_manual_upsampling(self.train_data)\n",
    "            # self.dev_data = self._test_manual_downsampling(self.dev_data)\n",
    "            return self.train_data, self.dev_data\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"File not found\")\n",
    "\n",
    "    def select_model(self, algorithm:str, hyperparameters:dict=None) -> object:\n",
    "        if algorithm == \"RandomForest\":\n",
    "            if hyperparameters is None:\n",
    "                self.model = RandomForestClassifier()\n",
    "            else:\n",
    "                self.model = RandomForestClassifier(**hyperparameters)\n",
    "        elif algorithm == \"LogisticRegression\":\n",
    "            if hyperparameters is None:\n",
    "                self.model = LogisticRegression()\n",
    "            else:\n",
    "                self.model = LogisticRegression(**hyperparameters)\n",
    "        elif algorithm == \"XGBoost\":\n",
    "            if hyperparameters is None:\n",
    "                self.model = GradientBoostingClassifier()\n",
    "            else:\n",
    "                self.model = GradientBoostingClassifier(**hyperparameters)\n",
    "        elif algorithm == \"RandomForest\":\n",
    "            if hyperparameters is None:\n",
    "                self.model = RandomForestClassifier()\n",
    "            else:\n",
    "                self.model = RandomForestClassifier(**hyperparameters)\n",
    "        elif algorithm == \"DecisionTree\":\n",
    "            if hyperparameters is None:\n",
    "                self.model = DecisionTreeClassifier()\n",
    "            else:\n",
    "                self.model = DecisionTreeClassifier(**hyperparameters)\n",
    "        elif algorithm == \"SVM\":\n",
    "            if hyperparameters is None:\n",
    "                self.model = SVC(probability=True)\n",
    "            else:\n",
    "                self.model = SVC(**hyperparameters)\n",
    "        elif algorithm == \"Baseline\":\n",
    "            self.model = DummyClassifier(strategy=\"most_frequent\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid algorithm\")\n",
    "        return self.model\n",
    "\n",
    "    def train_model(self, selected_features:list=None) -> object:\n",
    "        train_features, train_target = self._split_data_into_features_target(self.train_data)\n",
    "        self.model = self.model.fit(train_features[selected_features], train_target)\n",
    "        #self.model.fit(train_features.iloc[:,15:], train_target)\n",
    "        return self.model\n",
    "\n",
    "    def evaluate_model(self, selected_features:str) -> dict:\n",
    "        # predict, y_score = self._make_prediction(self.dev_data, selected_features)\n",
    "        dev_features, dev_target = self._split_data_into_features_target(self.dev_data)\n",
    "        predict = self.model.predict(dev_features[selected_features])\n",
    "        y_score = self.model.predict_proba(dev_features[selected_features])[:,1]\n",
    "        accuracy = accuracy_score(dev_target, predict)\n",
    "        precision = precision_score(dev_target, predict)\n",
    "        recall = recall_score(dev_target, predict)\n",
    "        auc_pr = eval_with_auc_and_pr_curve(dev_target, y_score)\n",
    "        auc_pr2 = eval_with_average_precision_score(dev_target, y_score)\n",
    "        f_beta_score = eval_with_f_beta_score(dev_target, predict)\n",
    "        self.metrics = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"auc_pr\": auc_pr,\n",
    "            \"auc_pr2\": auc_pr2,\n",
    "            \"f_beta_score\": f_beta_score\n",
    "        }\n",
    "        return self.metrics\n",
    "    \n",
    "    def export_model(self, model_path:str) -> None:\n",
    "        mlflow.sklearn.save_model(self.model, model_path)\n",
    "        return None\n",
    "    \n",
    "    def end_training(self) -> None:\n",
    "        mlflow.end_run()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    mlflow.set_experiment(\"CreditWorthinessTraining\")\n",
    "    with mlflow.start_run():\n",
    "        training_features = ['gender_F', 'gender_M', 'age_standardised', 'income',\n",
    "                             'family_size_large', 'family_size_single', 'family_size_small',\n",
    "                             'no_of_children',]\n",
    "        trainer = ModelTrainer(input_path=\"artifacts/data_prep/output\")\n",
    "        trainer.load_data(\"ml_train.parquet\", \"ml_dev.parquet\")\n",
    "        mlflow.log_param(\"data_downsampling_parameter\", [\"data_minority\",\"replace=False\",\n",
    "                                           \"n_samples = len(data_majority)\",\n",
    "                                           \"random_state=42\"])\n",
    "        # mlflow.log_param(\"train_data\", str(trainer.train_data))\n",
    "        # mlflow.log_param(\"dev_data\", str(trainer.dev_data))\n",
    "        trainer.select_model(\"RandomForest\")\n",
    "        mlflow.log_param(\"algorithm\", \"RandomForest\")\n",
    "        mlflow.log_param(\"hyperparameters\", trainer.model.get_params())\n",
    "        trainer.train_model(selected_features=training_features)\n",
    "        mlflow.log_param(\"selected_features\", training_features)\n",
    "        mlflow.log_param(\"model\", trainer.model)\n",
    "        metrics = trainer.evaluate_model(selected_features=training_features)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        trainer.export_model(\"artifacts/model_dev/model\")\n",
    "        trainer.end_training()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way of writing the training script is being explored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
